---
title: deeplogic assesment docs
draft: false
tags:
  - ML
---
***the project is Work in progress i have just submitted a basic working model the performance of the model isn't great as i am limited by time and hardware but i will highlight in this blog what can be done to greatly improve on those and several other metrics***


# Basic Overview:

![[Pasted image 20240225033127.png]]

The basic overveiw of the rag system is that the 
## IR-system:
- The IR system uses Faiss library for indexing and retriving answers
- There is no persistence yet, and an actual database saved on disk (something to work on)
- The indexings method is just simple ranking similarity scores, indexes like HNSW,
   or Tree index do well with IR systems for Retrival Augmented Generators  


## Embeddings model

- The word embeddings model used in the IR system is 'distilbert-base-uncased' however we can use a finetuned model on the domain specific dataset to achieve better results as done by these [kaggle competition winners](https://www.kaggle.com/competitions/kaggle-llm-science-exam/discussion/446422) 
- Similar to how they trained on the wikipedia corpus model could train on (for example) [aws docs](https://huggingface.co/datasets?search=aws)  as in the placebo pdf



## LLM:
- for the llm i have used python bindings for llama.cpp with the google gemma 2b parameter model of course there are a lot of shortcomings to it but i am limited by hardware and time so i went with it.
- Ideally there are a lot of LLM's on huggingface finetuned specifically on "Instruct Tuned datasets" which are used to train small sized llm's for RAG's




## Assessing the model
One great way of assessing the model is:
- To generate a high quality dataset on your specific task through the api's google gemini offers since their 1 million context window would be pretty good at answering long form inputs, and we could instruct it to format the output however we like (cite the place it found the answer)  
- Both the embeddings model and the IR system could be assessed pretty well through a devops pipeline like the ones mlflow offer or through Weights and Biases.
- The dataset generated by the gemini model could be used to evaluate against the results of the IR systems by setting ground truth values and then evaluating based on different metrics such as similarity scores , recall , f1 score etc. 




## Overall / Other improvements:
- ....